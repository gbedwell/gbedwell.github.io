---
title: nbconv
excerpt: An R package to evaluation negative binomial convolutions.
classes: wide
output:
  md_document:
    variant: gfm
    preserve_yaml: true
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_file = paste0(
        '/Users/gbedwell/Documents/github/gbedwell.github.io/_posts/', '2023-02-01', '-', 'nb-convolutions','.md'
      ),
      envir = globalenv()
    )
  })
---

```{r, include=FALSE}
date <- "2023-02-01"

base.dir <- "/Users/gbedwell/Documents/github/gbedwell.github.io/"
base.url <- "/"
dir.create(paste0("../figures/", date))
fig.path <- paste0("../figures/", date)

knitr::opts_chunk$set(
  fig.asp = 5/7,
  fig.width = 7,
  dpi = 300,
  fig.align = "center",
  out.width = "80%",
  comment = "#>",
  collapse = TRUE,
  dev = "png",
  base.dir = base.dir, 
  base.url = base.url,
  fig.path = fig.path
  )
```

### Introduction 

The negative binomial (NB) distribution is widely used to model count data whose variance is greater than expected given other discrete probability distributions. The ability to account for overdispersion in observed data using the NB distribution has led to its application across a wide range scientific disciplines as an alternative to Poisson models. Recently, I was interested in evaluating the sum of independent but not identically distributed negative binomial random variables (r.v.s). I discovered relatively quickly, however, that a straightforward solution to this problem doesn't really exist. The exact solutions that have been published by [Furman](https://www.researchgate.net/publication/23635822_On_the_convolution_of_the_negative_binomial_random_variables) and [Vellaisamy](https://www.jstor.org/stable/25662420) both have significant computational drawbacks. Approximate methods have also been [described](https://www.martinmodrak.cz/2019/06/20/approximate-densities-for-sums-of-variables-negative-binomials-and-saddlepoint/) for such sums, which largely alleviate the computational burdens of the exact methods, but at the potential cost of numeric accuracy. What really befuddled me, however, was the fact that I was unable to find any widely accessible package/tool/etc. that would let me easily apply any/all of these methods to actual data. As such, I wrote the R package that I felt was missing: <code>nbconv</code>. 

The current version of <code>nbconv</code> can be found on [CRAN](https://cran.r-project.org/package=nbconv) and the developmental version can be found on [GitHub](https://github.com/gbedwell/nbconv).

### Package description

<code>nbconv</code> was written with the same general syntax as other distribution functions in R. The package has 5 principal functions: <code>dnbconv()</code>, <code>pnbconv()</code>, <code>qnbconv()</code>, <code>rnbconv()</code>, and <code>nbconv_params()</code>. The first four of these return the mass function (PMF), distribution function (CDF), quantile function, and random deviates, respectively, for the convolution of NB r.v.s. The function <code>nbconv_params()</code> returns summary statistics of a given NB convolution based on its moments. The signatures for the 5 principal functions are:

```{r, eval = FALSE}
dnbconv(counts, mus, ps, phis, method = c("exact", "moments", "saddlepoint"), 
n.terms = 1000, n.cores = 1, tolerance = 1e-3, normalize = TRUE)

pnbconv(quants, mus, ps, phis, method = c("exact", "moments", "saddlepoint"),
n.terms = 1000, n.cores = 1, tolerance = 1e-3, normalize = TRUE)

qnbconv(probs, counts, mus, ps, phis, method = c("exact", "moments", "saddlepoint"),
n.terms = 1000, n.cores = 1, tolerance = 1e-3, normalize = TRUE)

rnbconv(mus, phis, ps, n.samp, n.cores = 1)

nbconv_params(mus, phis, ps)
```

The parameterization of the NB distribution used in <code>nbconv</code> is the same as the parameterization used by <code>stats::d/p/q/rnbinom()</code>. All of the <code>nbconv</code> functions take as input vectors of either constituent distribution means (<code>mus</code>) or probabilities of success (<code>ps</code>) and consitutent distribution dispersion parameters (<code>phis</code>, referred to as <code>size</code> in <code>stats</code>). 

The PMF, CDF, and quantile functions all require specification of the evaluation method. In <code>nbconv</code>, these are: Furman's exact equation (<code>method = "exact"</code>), a method of moments approximation (<code>method = "moments"</code>), and the saddlepoint approximation (<code>method = "saddlepoint"</code>). I'll avoid the gory mathematical details of the evaluation methods in this post, but a detailed description can be found [here](https://gbedwell.github.io/files/nbconv_background.pdf). To give credit where it is due, Martin Modrák's [blog post](https://www.martinmodrak.cz/2019/06/20/approximate-densities-for-sums-of-variables-negative-binomials-and-saddlepoint/) was my inspiration to include the saddlepoint approximation in <code>nbconv</code>. 

Other method-specific variables can also be user-defined in these functions. The variables <code>n.terms</code> and <code>tolerance</code> only pertain to evaluation via Furman's exact function and define 1) the number of terms included in the series and 2) how close the sum of the PMF of the mixture r.v. $K$ (see the [method descriptions](https://gbedwell.github.io/files/nbconv_background.pdf)) must be to 1 to be accepted, respectively. The threshold defined via <code>tolerance</code> serves as a way to ensure that the number of terms included in the series sufficiently describe the possible values of $K$. The variable <code>normalize</code> pertains to evaluation via the saddlepoint approximation and defines whether or not the saddlepoint density should be normalized to sum to 1, since the saddlepoint PMF is not guaranteed to do so. Evaluation of the mass, distribution, and quantile functions via the exact or the saddlepoint methods, as well as generation of random deviates via <code>rnbconv()</code>, can be parallelized by setting <code>n.cores</code> to a value greater than 1. It should be noted, however, that for the exact function, only evaluation of the PMF, and \emph{not} evaluation of the recursive parameters, are parallelized. Because of this, CPU time for evaluation of the exact function is linearly related to the number of terms included in the series. <code>rnbconv()</code> and <code>nbconv_params()</code> work independently of the evaluation methods described above. The variable <code>n.samp</code> in <code>rnbconv()</code> defines the number of random deviates to be sampled from the target convolution.

### Examples

To demonstrate general use of <code>nbconv</code> functions, I'll generate some sample data. I'll use the gamma distribution to ensure that $\mu ≥ 0$ and $\phi > 0$.

```{r}
library(nbconv)

set.seed(1234)
mus <- rgamma(n = 25, shape = 3.5, scale = 3.5)

set.seed(1234)
phis <- rgamma(n = 25, shape = 2, scale = 4)
```

Summary statistics of the convolution can be informative as to what methods might or might not work well with our data. I won't go into too much detail here, but in general, the exact method provides the most accurate results but at what can be a steep computational cost. For convolutions of wildly different NB distributions and/or highly overdispersed distributions, the influence of the mixture distribution on the shape of the convolution grows. When this happens, the number of terms included in the series generally has to increase as well. Because the exact method depends on recursive parameters, this means linearly increasing computation time. However, in instances where the convolution is largely symmetric and/or doesn't exhibit a large degree of kurtosis, the method of moments and saddlepoint approximations work pretty well. Anecdotally, the saddlepoint approximation is a little bit more robust to skewness and kurotosis than the method of moments approximation, but I won't explore this point in any more detail here.

```{r}
nbconv_params( mus = mus, phis = phis )
```

The output of <code>nbconv_params()</code> tells us that the convolution of the sample NB r.v.s is approximately symmetric and doesn't exhibit much tailing. Because of this, we could probably get away with using any of the three evaluation methods. For the purposes of demonstration, however, I'll go ahead and use them all. I'll additionally calculate empirical probability masses from random deviates sampled using <code>rnbconv()</code> to serve as reference data.

```{r}
samps <- rnbconv( mus = mus, phis = phis, n.samp = 1e6, n.cores = 1 )

empirical <- stats::density(x = samps, from = 0, to = 500, n = 500 + 1)$y

exact <- dnbconv( mus = mus, phis = phis, counts = 0:500, 
                  method = "exact", n.terms = 1000, n.cores = 1 )

moments <- dnbconv( mus = mus, phis = phis, counts = 0:500, method = "moments" )

saddlepoint <- dnbconv( mus = mus, phis = phis, counts = 0:500, 
                        method = "saddlepoint", n.cores = 1, normalize = TRUE )
```

For easier visualization, I'll combine the four calculated probability mass vectors into a single long data frame.

```{r}
df <- data.frame( empirical, exact, moments, saddlepoint )

df$count <- c( 0:500 )

df <- df |>
  tidyr::pivot_longer( cols = !count, names_to = "method", values_to = "probability") |>
  dplyr::arrange( method, count )

df$method <- factor( df$method, levels = c("empirical", "exact", "moments", "saddlepoint") )
```

```{r}
library( ggplot2 )

ggplot(data = df,
             aes(x = count, y = probability , fill = method ) ) +
  geom_area(data = df[df$method == "empirical",],
              aes(x = count, y = probability ),
              color = "gray50", fill="gray50",
              inherit.aes = FALSE, alpha = 0.5 ) +  
  geom_point(shape = 21, size = 2, color = "#00000000") +
  scale_fill_manual(values = c("gray50","darkblue","darkred","darkgreen"),
                     labels = c("Empirical", "Exact", "Moments", "Saddlepoint" ),
                     guide = guide_legend(title.position = "top",
                                          title.hjust = 0.5,
                                          override.aes = list( size = 2.5 ) ) ) +
  theme_bw() +
  theme(axis.title = element_text(size = 16),
        axis.text = element_text(size = 14),
        legend.position = "top",
        legend.title = element_blank(),
        legend.key.size = unit(3, "point"),
        panel.spacing = unit(1, "lines") ) +
  labs(x = "Counts", y = "Probability")
```

Visualizing the data, we can see that all three methods do indeed appear describe the empirical distribution well. This will not always be the case! I strongly encourage users of <code>nbconv</code> to pay attention to the summary statistics of the target convolution and to compare the evaluated distribution to random deviates!

Hopefully this brief example effectively demonstrates the general workflow of <code>nbconv</code>. If you have any comments or questions, please reach out!











